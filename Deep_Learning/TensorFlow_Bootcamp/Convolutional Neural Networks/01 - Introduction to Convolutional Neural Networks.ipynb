{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Review of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now understand how to perform a calculation in a neuron:\n",
    "\n",
    "`z = W * x + b`\n",
    "\n",
    "`a = act_function(z)`\n",
    "\n",
    "We also know that there are several activation functions available, such as Perceptrons, Sigmoid, Tanh, ReLU (some others will be discussed shortly).\n",
    "\n",
    "Connecting single neurons will result in a Neural Network:\n",
    "\n",
    "- Input Layer\n",
    "- Hidden Layers\n",
    "- Output Layer\n",
    "\n",
    "More layers imply more abstraction.\n",
    "\n",
    "In order to learn we need some measurement of error and feedback. We use a cost function that can be:\n",
    "\n",
    "- Quadratic\n",
    "- Cross Entropy\n",
    "\n",
    "Once we have the measurement of error, we need to minimize it by choosing the correct weight and bias values. We use Gradient Descent to find those optimal values.\n",
    "\n",
    "We can then backpropagate the gradient descent through multiple layers, from the output layer back to the input layer. We also know of dense layers (fully connected to the next layer), but later on we will introduce *softmax* layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Theory Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights Value Initialization\n",
    "So far we have initialised values for weights using random values, but that isn't the only option:\n",
    "\n",
    "- Zeros: No randomness, and not a great choice\n",
    "- Random distribution near zero: not optimal, and distorts the activation function\n",
    "\n",
    "But there are more:\n",
    "\n",
    "- Xavier (Glorot) initialization: Uniform/Normal. Draw weights from a distribution with zero mean and specific variance 1/n, where n is the number of neurons.\n",
    "\n",
    "### Learning Rate and Batch Size\n",
    "\n",
    "The learning rate defines the step size during Gradient Descent, and the Batch Size allows us to take a small sample of the data to apply Stochastic Gradient Descent, as a trade-off between the representativeness of the data and the training time.\n",
    "\n",
    "The Second Order Behavior of the Gradient Descent allows us to adjust our learning rate based off the rate of descent, meaning the learning rate is variable instead of a fixed value:\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- **Adam**\n",
    "\n",
    "Larger steps at the beginning, and eventually going to smaller step sizes as we approach the optimal value.\n",
    "\n",
    "### Unstable or Vanishing Gradients\n",
    "\n",
    "As we increase the number of layers in a network, the layers towards the input will be affected less by the error calculation ocurring at the output as you backpropagate through the network, specially if the network is very deep. Initialisation and Normalisation will help mitigate this processed, known as **vanishing Gradients**.\n",
    "\n",
    "### Overfitting vs Underfitting a Model\n",
    "\n",
    "If we get a larger error on the test data compared to the train data, then we are **underfitting**.\n",
    "\n",
    "If the train error is very low, and test data has a huge error, then we are **overfitting**. Fitting too well makes the model meaningless for predictions.\n",
    "\n",
    "Need some sort of balance. With potentially hundreds of parameters in a deep learning neural network, the possibility of overfitting is very high!\n",
    "\n",
    "There are a few ways to help mitigate this issue:\n",
    "\n",
    "- L1/L2 Regularization: Adds a penalty for larger weights in the model (not unique to neural networks).\n",
    "- Dropout: During training, remove neurons randomly, so that the network doesn't rely on any particular neuron.\n",
    "- Expanding data: Adding noise, tilting images, low white noise to sound data, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
