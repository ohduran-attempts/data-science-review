{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out gensim library if you are interested in Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how to work with time series of data, let's take a look at words.\n",
    "\n",
    "For example sentences can be\n",
    "\n",
    "`[\"Hi\", \"how\", \"are\", \"you\"]`\n",
    "\n",
    "In classic NLP, words are typically replaced by numbers indicating some frequency relationship to their documents. In doing this, we lose information about the relationship between the words themselves. That is **count-based approach**. We are now going to look at **predictive-based** approaches, where neighbouring words are predicted based on a vector space.\n",
    "\n",
    "## Word2Vec model\n",
    "\n",
    "The goal of Word2Vec model is to learn word embeddings by modelling each word as a vector in n-dimensional space. We use word embeddings because text, on count-based approach, we treated words as symbols, with no relationship to its surroudings the way we do with images or audio.\n",
    "\n",
    "Word2Vec creates vector spaced models that represent words in a continuous vector space. With words as vectors, we can perform vector mathematics on words, like check their similarity, add or substract vectors, etc.\n",
    "\n",
    "At the start of training, each embedding is random, but through backpropagation the model will adjust the value of each word vector in the given number of dimensions. More dimensions means more training time, but also more \"information\" per word. What ends up happening is that each dimension will represent some sort of idea.\n",
    "\n",
    "Similar words will find their vectors closer together; even more impressive, the model may produce axes that represents concepts such as gender, verbs, singular or plural, etc... Male-Female, Verb tense, Country-Capital. Doing addition or substraction in this model may mean going from male to female, changing the verb tense, etc.\n",
    "\n",
    "### Continuos Bag of Words (CBOW)\n",
    "\n",
    "Takes a sentence such as *the dog chews the **bone*** and then finds the target word bone.\n",
    "\n",
    "### Skip-Gram\n",
    "\n",
    "Takes a word such as **bone** and predicts the sentence *the dog chews the bone*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Contrastive Training\n",
    "\n",
    "In CBOW, given the sentence *the dog chews the `w[t]`*, we are looking for the target word, versus the collection of words we call noise words. We will succeed if we find the word most likely to have the sentence as a target.\n",
    "\n",
    "The target word is predicted by maximizing:\n",
    "\n",
    "`J = log(Q[theta](D=1 | w[t], h) + k * E[log Q[theta](D=0 | w[n], h)]`\n",
    "\n",
    "Q in binary logistic regression is the probability that the word w is in the context h in the dataset D parameterized by theta.\n",
    "\n",
    "The basic idea is that J represents the cross entropy between w being the target word and the rest of the words not being the target word. The goal is to assign a high probability to correct words and low probability to noise words.\n",
    "\n",
    "Once we have vectors for each word we can visualize relationships by reducing the dimensions from 150 to 2 using t-Distributed Stochastic Neighbour Embedding.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
