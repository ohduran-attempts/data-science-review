{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've used Neural Networks to solve classification and regression problems, but we still haven't seen how Neural Networks can deal with sequence information, such as a time series.\n",
    "\n",
    "For this, we use RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Sequences\n",
    "\n",
    "- Time Series data\n",
    "- Sentences within NPL\n",
    "- Audio\n",
    "- Car Trajectories\n",
    "- Music\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine a sequence `[1, 2, 3, 4, 5, 6]`. Would you be able to predict a similar sequence shifted one time step into the future? Sure, it's `[2,3,4,5,6,7]`!\n",
    "\n",
    "Instead of Normal Neural Networks, we use Recurrent Neural Networks. RNNs are special in the sense that **RNNs send output back to itself, as well as to the next layer**. Input as t, t-1 and t+1 produce a different response: unrolling the recurrent network.\n",
    "\n",
    "Cells that are a function of inputs from previous time steps are also known as *memory cells*. RNNs are also flexible in their inputs and outputs, for both sequences and single vector values.\n",
    "\n",
    "We can also create layers of RNNs that feedback to each other and to the complete layer: given that we are passing information from different points in time, we can say that **RNNs have some sort of memory**.\n",
    "\n",
    "We can now perform\n",
    "- Sequence input to sequence ouput\n",
    "- Sequence to vector (sentiment analysis)\n",
    "- Vector to sequence (image and get a description of the image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation goes backwards from the output to the input layer, propagating the error gradient. For deeper networks, issues can arise from backpropagation, vanishing and exploding gradients!\n",
    "\n",
    "As you go back to the 'lower' layers, gradients often get smaller, eventually causing weights to never change at lower levels. The opposite can also occur, gradients explode on the way back, causing issues.\n",
    "\n",
    "### Why does it happen?\n",
    "\n",
    "Recall the sigmoid function? When you get close to 0 or 1, the slope is almost zero. This chain rule of backpropagation means that the gradient get's exponentially small while the slope decrease really slowly due to its proximity to the edges.\n",
    "\n",
    "- Use a different activation function: ReLU doesn't saturate positive values, and the 'leaky', exponential, different versions of the ReLU.\n",
    "\n",
    "- Batch normalization: The model will normalize each batch using the batch mean and standard deviation.\n",
    "\n",
    "- Gradient clipping: Gradients are cut off before reaching a predetermined limit.\n",
    "\n",
    "Many of the solutions previously presented can also apply for RNNs, but the length of time series could slow down training. A possible solution would be to just shorten the time steps used for prediction, but this makes the model worse at predicting longer trends.\n",
    "\n",
    "Another issue is that after a while, the network will begin to 'forget' the first inputs, as information is lost at each step going through the RNN.\n",
    "\n",
    "### LSTM\n",
    "Long Short-Term Memory cells were developed to address these RNNs issues in 1996. A new input called 'cell state' is created, and passed as an output too. \n",
    "\n",
    "1. **Forget Gate Layer**: Decide what information are we throwing away from the cell state. It completely replicates a normal neural network into a sigmoid layer: 1 means keep, 0 means forget. `f(t) = sigma(W(f) * [h(t-1), x(t)] + b(f)` \n",
    "2. **Keep Gate Layer**: Decide what information are we keeping on the cell state.\n",
    "   - Input Gate Layer: Sigmoid, `i(t) = sigma(W(i) * [h(t-1), x(t)] + b(i)`\n",
    "   - New Candidate Values: Hyperbolic tangent Layer, `Ĉ(t) = tanh(W(C) * [h(t-1), x(t)] + b(C)`\n",
    "\n",
    "The old cell state is updated using these functions using `C(t) = f(t) * C(t-1) + i(t) * Ĉ(t)`.\n",
    "\n",
    "3. **Feedback Layer**: The output that will feed back to the layer is `o(t) = sigma(W(o) * [h(t-1), x(t)] + b`. The final output is `h(t) = o(t) * tanh(C(t))`.\n",
    "\n",
    "There are different versions like the 'peepholes' one, where the functions f, i, Ĉ include C(t-1) in their inputs.\n",
    "\n",
    "### GRU\n",
    "\n",
    "Another version of LTSM is the Gated Recurrent Unit, or GRU, where there is a simplification of LTSM by combining the forget and input gate in what is called an *update* gate. It also merges the cell state and hidden state.\n",
    "\n",
    "Simpler means growing in popularity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "Fortunately, TensorFlow comes with these neuron models built into a nice API, making it easy to swap them in and out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
