{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we launch straight into neural networks, we need to understand the individual components first, such as the single \"neuron\".\n",
    "\n",
    "Artificial Neural Networks (ANN) have a basis in biology, as an attempt to mimic biological neurons.\n",
    "\n",
    "We have inputs and an ouput. Inputs are multiplied by a weight (initialised by a random numbers), and the results are passed to an activation function, that uniquely determines the ouput. One of the inputs may be something called \"bias\", that accounts for cases where the activation function does not determine the result of a certain case of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`output = activation_function(sum(weights * inputs) + biases)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning will be the result of chaining several ANNs altogether, distributed as 'layers': an input layer (taking real values from the data), and an ouput layer (the final estimate of the ouput), with some layers inbetween called hidden layers (a network is considered \"deep\" if there is at least 1 hidden layer).\n",
    "\n",
    "As you go forward through more layers, the level of abstraction increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of activation function is a simple function that outputs 0 if output is negative and 1 if output is positive. It's a pretty dramatic function, since small changes aren't reflected. A more dynamic function would be more useful, such as the **sigmoid function**.\n",
    "\n",
    "`sigmoid(x) = 1 / (1 + exp(-x) )` (image: 0 to 1)\n",
    "\n",
    "Changing the activation function used can be beneficial!\n",
    "\n",
    "Some other activation functions that we are going to see are:\n",
    "\n",
    "- Hyperbolic Tangent: `tanh(x) = (exp(x) + exp(-x)) / (exp(x) - exp(-x))` (similar to sigmoid but the image goes from -1 to 1)\n",
    "- Rectified Linear Unit (ReLU): `RELU(x) = max(0, x)` (it is the output when positive, 0 when negative)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
