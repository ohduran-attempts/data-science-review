{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we launch straight into neural networks, we need to understand the individual components first, such as the single \"neuron\".\n",
    "\n",
    "Artificial Neural Networks (ANN) have a basis in biology, as an attempt to mimic biological neurons.\n",
    "\n",
    "We have inputs and an ouput. Inputs are multiplied by a weight (initialised by a random numbers), and the results are passed to an activation function, that uniquely determines the ouput. One of the inputs may be something called \"bias\", that accounts for cases where the activation function does not determine the result of a certain case of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`output = activation_function(sum(weights * inputs) + biases)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning will be the result of chaining several ANNs altogether, distributed as 'layers': an input layer (taking real values from the data), and an ouput layer (the final estimate of the ouput), with some layers inbetween called hidden layers (a network is considered \"deep\" if there is at least 1 hidden layer).\n",
    "\n",
    "As you go forward through more layers, the level of abstraction increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of activation function is a simple function that outputs 0 if output is negative and 1 if output is positive. It's a pretty dramatic function, since small changes aren't reflected. A more dynamic function would be more useful, such as the **sigmoid function**.\n",
    "\n",
    "`sigmoid(x) = 1 / (1 + exp(-x) )` (image: 0 to 1)\n",
    "\n",
    "Changing the activation function used can be beneficial!\n",
    "\n",
    "Some other activation functions that we are going to see are:\n",
    "\n",
    "- Hyperbolic Tangent: `tanh(x) = (exp(x) + exp(-x)) / (exp(x) - exp(-x))` (similar to sigmoid but the image goes from -1 to 1)\n",
    "- Rectified Linear Unit (ReLU): `RELU(x) = max(0, x)` (it is the output when positive, 0 when negative)\n",
    "\n",
    "RELU and Hyperbolic tend to have the best performance, and they are built in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we evaluate performance of a neuron? Let's agree on some notation:\n",
    "\n",
    "y: represents the true value of the test case\n",
    "\n",
    "a: represents neuron's prediction of the test case\n",
    "\n",
    "n: number of test cases\n",
    "\n",
    "f(x) = a\n",
    "\n",
    " - Quadratic Cost Function: `C = sum(y - a)^2 / n` - Large errors are more prominent, but calculation can cause a slowdown.\n",
    " - Cross Entropy: `C=(-1 / n) * sum(y * ln(a) + (1-y) * ln(1-a))` - Allows for faster learning; the larger the difference, the faster the neuron can learn.\n",
    "\n",
    "\n",
    "But what is actually \"learn\"? We need to figure out how we can use our neurons and the measurement of error (cost function) to attempt to correct our prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent** is an optimization algorithm for finding the minimum of a function: to find a local minimum, we take steps proportional to the negative of the gradient.\n",
    "\n",
    "Using gradient descent we can figure out the best parameters for minimizing our cost, finding the best values for the weights of the neuron inputs.\n",
    "\n",
    "**Backopropagation** is used to calculate the error contribution of each neuron after a batch of data is processed. It relies heavily on the chain rule to go back through the network and calculate these errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
